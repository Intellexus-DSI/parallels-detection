# Full Pipeline Configuration
# This file configures all three stages of the parallel text detection pipeline

# ============================================================================
# STAGE 1: SEGMENTATION
# ============================================================================
segmentation:
  # Input file (JSONL format with Tibetan text)
  input_file: "data/05_clean_data/00_tibetan/Tibetan_1.jsonl"
  
  # Output directory for segmented files
  output_dir: "data/segmented_output"
  
  # Segmentation engine: "botok" (accurate) or "regex" (fast)
  engine: "regex"
  
  # Minimum syllables per segment
  min_syllables: 4
  
  # Segmentation mode: true = overlapping, false = exclusive
  use_overlapping: true
  
  # Overlapping mode parameters (only used if use_overlapping = true)
  overlap_max_atoms: 8        # Maximum atoms per span
  overlap_min_chars: 8        # Minimum characters per span
  overlap_max_chars: 350      # Maximum characters per span
  max_spans_per_line: 300     # Safety cap on spans per line

# ============================================================================
# STAGE 2: INDEXING
# ============================================================================
indexing:
  # Input directory (from segmentation stage)
  input_dir: "data/segmented_output/overlapping/Full_Files"
  
  # Output directory for embeddings and consolidated segments
  output_dir: "data"
  
  # Embedding model
  model: "sentence-transformers/all-MiniLM-L6-v2"
  # Other options:
  # - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  # - "openai/text-embedding-ada-002"
  
  # Batch size for embedding generation
  batch_size: 32
  
  # Normalize embeddings for cosine similarity
  normalize: true
  
  # Output file names
  embeddings_file: "embeddings.npy"
  segments_file: "full_segmentation.xlsx"

# ============================================================================
# STAGE 3: DETECTION
# ============================================================================
detection:
  # Input files (from indexing stage)
  segments_csv: "data/full_segmentation.xlsx"
  embeddings_path: "data/embeddings.npy"
  
  # Output path for parallel matches
  output_path: "output/parallels.csv"
  
  # Matching configuration
  matching:
    # Strategy: "threshold" or "knn"
    strategy: "threshold"
    
    # For threshold strategy: minimum similarity score (0.0 to 1.0)
    threshold: 0.85
    
    # For knn strategy: number of nearest neighbors
    k: 10
    
    # Optional minimum threshold for knn strategy
    min_threshold: 0.0
  
  # Processing options
  processing:
    # Batch size for processing segments
    batch_size: 1000
    
    # Normalize embeddings (should match indexing stage)
    normalize_embeddings: true
  
  # Output options
  output:
    # Format: "csv", "parquet", or "json"
    format: "csv"
    
    # Include segment text in output
    include_text: true

# ============================================================================
# AZURE CONFIGURATION (Optional - for downloading source files)
# ============================================================================
azure:
  account_url: "https://intlxresearchstorage.file.core.windows.net"
  sas_token: ""  # Add your SAS token here
  share_name: "intlx-gpu-fs"
  file_path: "data/05_clean_data/00_tibetan/Tibetan_1.jsonl"
