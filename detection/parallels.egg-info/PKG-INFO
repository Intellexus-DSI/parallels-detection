Metadata-Version: 2.4
Name: parallels
Version: 0.1.0
Summary: Find parallel text segments across a corpus using FAISS
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: faiss-cpu>=1.7.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: tqdm>=4.60.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"

# Parallels Pipeline

A pipeline for finding parallel text segments across a corpus of Tibetan/Sanskrit texts using FAISS for efficient similarity search.

## Overview

This pipeline takes segmented text embeddings and finds "parallels" - segments from different texts that are semantically similar. It uses FAISS for efficient vector similarity search and supports multiple matching strategies.

## Features

- **Efficient Search**: Uses FAISS IndexFlatIP for exact cosine similarity search
- **Multiple Strategies**: Threshold-based matching or K-Nearest Neighbors
- **Cross-Text Only**: Automatically filters out matches from the same source text
- **Configurable**: YAML configuration for all parameters
- **Scalable**: Batch processing for memory efficiency

## Installation

```bash
pip install -r requirements.txt
```

## Project Structure

```
parallels/
├── __init__.py
├── config.py              # Configuration management (Pydantic)
├── models.py              # Data classes (Segment, ParallelMatch)
├── pipeline.py            # Main orchestrator
├── cli.py                 # Command-line interface
│
├── data/
│   ├── __init__.py
│   ├── segment_store.py   # Load CSV metadata + embeddings
│   └── output_writer.py   # Write results (CSV, Parquet, JSON)
│
├── index/
│   ├── __init__.py
│   └── faiss_index.py     # FAISS wrapper for similarity search
│
└── matching/
    ├── __init__.py
    ├── base.py            # Abstract MatcherStrategy
    ├── threshold_matcher.py   # Similarity threshold matching
    ├── knn_matcher.py     # K-nearest neighbors matching
    └── filters.py         # Cross-text filter, deduplication
```

## Input Files

### 1. Segments CSV

A CSV file containing segment metadata with these columns:

| Column | Description |
|--------|-------------|
| `Segmented_Text` | Original Tibetan/Sanskrit text |
| `Segmented_Text_EWTS` | EWTS transliteration |
| `Length` | Character length of segment |
| `File_Path` | Source file path (used as text identifier) |
| `Title` | Document title |
| `Source_Line_Number` | Line number in source |
| `Sentence_Order` | Order within the line |
| `Start_Index` | Start character index |
| `End_Index` | End character index |

### 2. Embeddings File

A NumPy `.npy` file containing embeddings:
- Shape: `(N, 768)` where N = number of segments
- Type: `float32`
- Row order must match the CSV row order

## Configuration

Create a `config.yaml` file:

```yaml
# Input paths
segments_csv: "data/segments.csv"
embeddings_path: "data/embeddings.npy"

# Output path
output_path: "output/parallels.csv"

# Matching configuration
matching:
  strategy: "threshold"    # Options: "threshold" or "knn"
  threshold: 0.85          # For threshold strategy (0.0 to 1.0)
  k: 10                    # For knn strategy
  min_threshold: 0.0       # Optional minimum threshold for knn

# Processing options
processing:
  batch_size: 1000         # Segments per batch
  normalize_embeddings: true

# Output options
output:
  format: "csv"            # Options: "csv", "parquet", "json"
  include_text: true       # Include segment text in output
```

## Usage

### Command Line

```bash
# Using config file
python -m parallels.cli --config config.yaml

# Override specific options
python -m parallels.cli --config config.yaml --threshold 0.9

# Direct arguments (no config file)
python -m parallels.cli \
    --segments data/segments.csv \
    --embeddings data/embeddings.npy \
    --output output/parallels.csv \
    --strategy threshold \
    --threshold 0.85
```

### Python API

```python
from parallels.config import Config
from parallels.pipeline import ParallelsPipeline

# Load configuration
config = Config.from_yaml("config.yaml")

# Run pipeline
pipeline = ParallelsPipeline(config)
pipeline.run()
```

## Output Format

The output CSV contains:

| Column | Description |
|--------|-------------|
| `segment_a_id` | Row index of first segment |
| `segment_b_id` | Row index of second segment |
| `similarity` | Cosine similarity score (0.0 to 1.0) |
| `title_a` | Title of text A |
| `title_b` | Title of text B |
| `ewts_a` | EWTS text of segment A |
| `ewts_b` | EWTS text of segment B |
| `file_path_a` | Source file of segment A |
| `file_path_b` | Source file of segment B |

## Matching Strategies

### Threshold Matcher

Finds all segment pairs with similarity above a threshold.

- **Use when**: You want all matches above a quality threshold
- **Pros**: Complete coverage, quality controlled
- **Cons**: May produce many results for low thresholds

### KNN Matcher

Finds the top-k most similar segments for each segment.

- **Use when**: You want a fixed number of matches per segment
- **Pros**: Predictable output size, finds best matches
- **Cons**: May include low-quality matches if k is large

## Performance

For 200K segments with 768-dimensional embeddings:

| Metric | Value |
|--------|-------|
| Memory usage | ~2-3 GB |
| Index build time | ~2-3 seconds |
| Search time | ~5-10 minutes |

## Algorithm Details

### Cosine Similarity via FAISS

FAISS `IndexFlatIP` computes inner product. For cosine similarity:

1. L2-normalize all embedding vectors
2. Build index with normalized vectors
3. Inner product of normalized vectors = cosine similarity

### Deduplication

Each parallel pair is stored once: `(min(a,b), max(a,b))` to avoid duplicates like (A,B) and (B,A).

### Cross-Text Filtering

Matches are filtered to ensure `File_Path` of segment A differs from segment B.
